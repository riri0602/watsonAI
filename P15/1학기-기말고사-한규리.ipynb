{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AI_finalTest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_YHvLkqjCrc"
      },
      "source": [
        "#IBM 왓슨 인공지능 과정 기말고사\n",
        "### 일자 : 2021년 6월 9일"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb_TX0mojY_5"
      },
      "source": [
        "##이름 : '한규리'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIe4C3EfkTDV"
      },
      "source": [
        "#문제 1 : Gensim을 이용한 Word2Vec(15점)\n",
        "### 다음 문제는 gensim API를 사용하여 Word2Vec으로 word embedding 모델을 구현하는 문제입니다. 여러분이 실습하였던 네이버 영화평 과제와 유사합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypnrK__LrPUd"
      },
      "source": [
        "# imports needed and set up logging\n",
        "import gzip\n",
        "import gensim \n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLkGIxkmTB3R"
      },
      "source": [
        "word embedding 모델을 만들 데이터셋을 다운로드받습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIdqVu1KmRNr",
        "outputId": "9b0f52e2-7af9-402b-f5c1-78e933cd205e"
      },
      "source": [
        "! wget --quiet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1W-UgYO0FmmnKYTj2oKaGlsfxCcmIU1y9' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1W-UgYO0FmmnKYTj2oKaGlsfxCcmIU1y9\" -O reviews_data.txt.gz && rm -rf /tmp/cookies.txt\n",
        "data_file=\"reviews_data.txt.gz\"\n",
        "\n",
        "with gzip.open ('reviews_data.txt.gz', 'rb') as f:\n",
        "    for i,line in enumerate (f):\n",
        "        print(line)\n",
        "        break"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\"Oct 12 2009 \\tNice trendy hotel location not too bad.\\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\\t\\r\\n\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1T6Z-46STIQV"
      },
      "source": [
        "다운로드 받은 파일을 한 줄 씩 읽습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVe9Mgutmda-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5d579f9-c5e4-4647-d02e-39562980363c"
      },
      "source": [
        "def read_input(input_file):\n",
        "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
        "    \n",
        "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
        "    \n",
        "    with gzip.open (input_file, 'rb') as f:\n",
        "        for i, line in enumerate (f): \n",
        "\n",
        "            if (i%10000==0):\n",
        "                logging.info (\"read {0} reviews\".format (i))\n",
        "            # do some pre-processing and return a list of words for each review text\n",
        "            yield gensim.utils.simple_preprocess (line)\n",
        "\n",
        "# read the tokenized reviews into a list\n",
        "# each review item becomes a serries of words\n",
        "# so this becomes a list of lists\n",
        "documents = list (read_input (data_file))\n",
        "logging.info (\"Done reading data file\")    "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-09 04:01:29,528 : INFO : reading file reviews_data.txt.gz...this may take a while\n",
            "2021-06-09 04:01:29,530 : INFO : read 0 reviews\n",
            "2021-06-09 04:01:31,453 : INFO : read 10000 reviews\n",
            "2021-06-09 04:01:33,377 : INFO : read 20000 reviews\n",
            "2021-06-09 04:01:35,585 : INFO : read 30000 reviews\n",
            "2021-06-09 04:01:37,575 : INFO : read 40000 reviews\n",
            "2021-06-09 04:01:39,787 : INFO : read 50000 reviews\n",
            "2021-06-09 04:01:41,846 : INFO : read 60000 reviews\n",
            "2021-06-09 04:01:43,615 : INFO : read 70000 reviews\n",
            "2021-06-09 04:01:45,237 : INFO : read 80000 reviews\n",
            "2021-06-09 04:01:46,954 : INFO : read 90000 reviews\n",
            "2021-06-09 04:01:48,969 : INFO : read 100000 reviews\n",
            "2021-06-09 04:01:50,593 : INFO : read 110000 reviews\n",
            "2021-06-09 04:01:52,231 : INFO : read 120000 reviews\n",
            "2021-06-09 04:01:53,898 : INFO : read 130000 reviews\n",
            "2021-06-09 04:01:55,699 : INFO : read 140000 reviews\n",
            "2021-06-09 04:01:57,378 : INFO : read 150000 reviews\n",
            "2021-06-09 04:01:59,092 : INFO : read 160000 reviews\n",
            "2021-06-09 04:02:00,746 : INFO : read 170000 reviews\n",
            "2021-06-09 04:02:02,488 : INFO : read 180000 reviews\n",
            "2021-06-09 04:02:04,736 : INFO : read 190000 reviews\n",
            "2021-06-09 04:02:06,598 : INFO : read 200000 reviews\n",
            "2021-06-09 04:02:08,408 : INFO : read 210000 reviews\n",
            "2021-06-09 04:02:10,201 : INFO : read 220000 reviews\n",
            "2021-06-09 04:02:11,848 : INFO : read 230000 reviews\n",
            "2021-06-09 04:02:13,597 : INFO : read 240000 reviews\n",
            "2021-06-09 04:02:15,313 : INFO : read 250000 reviews\n",
            "2021-06-09 04:02:16,244 : INFO : Done reading data file\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkoZ1y6eNQlp"
      },
      "source": [
        "##문제1-1. Word2Vec API를 사용하여 다운로드받은 파일에 대한 word embedding 모델을 트레이닝시키십시오.(5점)\n",
        "Word2Vec의 파라미터는 다음과 같이 적용하십시오. \n",
        "size=150, window=10, min_count=2, workers=10, iter=1\n",
        "\n",
        "트레이닝 시키는데 2분 가량 소요됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i55O0qECmiLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e307169-cbd7-46f9-8ace-90cb8e2840ea"
      },
      "source": [
        "from gensim.models import word2vec\n",
        "%time model = word2vec.Word2Vec(size=150, window=10, min_count=2, workers=10, iter=1)\n",
        "model.init_sims(replace=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-09 04:22:17,913 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
            "2021-06-09 04:22:17,917 : INFO : precomputing L2-norms of word weight vectors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 2.6 ms, sys: 0 ns, total: 2.6 ms\n",
            "Wall time: 2.24 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMiqYJmLnsJw"
      },
      "source": [
        "##문제1-2. gensim의 API를 사용하여 'dirty'의 동의어들을 찾아내십시오. \n",
        "아래 링크의 네이버 영화평 코드를 참조하십시오.\n",
        "\n",
        "참조 : https://colab.research.google.com/drive/1kY7mpCXo_5RDB-WpTO_4bARDFaZB_Vyi#scrollTo=45SmN7RK7hJb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Lne2vO8mjfn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        },
        "outputId": "bfc89424-91dc-4a6d-bef9-54d09cc7f176"
      },
      "source": [
        "# dirty와 가장 similar한 단어들을 출력하십시오.\n",
        "print(model.wv.most_similar(positive=[\"dirty\"]))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-3f012018412b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# dirty와 가장 similar한 단어들을 출력하십시오.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dirty\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'dirty' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GV48dmBwQik4"
      },
      "source": [
        "##문제1-3. gensim의 API를 사용하여 'dirty'와 'unclean'간의 코사인 유사도(cosine similarity)를 출력하십시오.\n",
        "다음 링크의 similarity 계산 예제를 참고하십시오.\n",
        "\n",
        "참고 https://radimrehurek.com/gensim_3.8.3/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "8U-RaLt7msul",
        "outputId": "7f2e5c52-b03c-4d92-afd7-be0be3631e5b"
      },
      "source": [
        "# similarity between 'dirty' and 'unclean'\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "import numpy as np\n",
        "def cos_sim(A, B):\n",
        "       return dot(A, B)/(norm(A)*norm(B))\n",
        "print(cos_sim('dirty', 'unclean'))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UFuncTypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-0d0ef93ab997>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dirty'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unclean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-0d0ef93ab997>\u001b[0m in \u001b[0;36mcos_sim\u001b[0;34m(A, B)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcos_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dirty'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'unclean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mUFuncTypeError\u001b[0m: ufunc 'multiply' did not contain a loop with signature matching types (dtype('<U7'), dtype('<U7')) -> dtype('<U7')"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU9JKdNvrP2w"
      },
      "source": [
        "#문제 2: RNN을 이용한 한국어 문장 생성기 코딩(15점)\n",
        "### 다음 코드 중 your_code_here 부분을 완성하십시오. P10-1의 \"글쓰는 인공지능2 실습\"에 사용된 코드를 참고하십시오."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZDuqyL_rmYG"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkcsdQrnVAY2"
      },
      "source": [
        "학습할 입력 문장을 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32jTHO15VAzN"
      },
      "source": [
        "text=\"\"\"특히 최근 수도권에서는 교회 소모임 참석자에 이어 이들 가족과 지인으로 번지는 2차 감염 사례가 증가하고 있습니다. 중앙재난안전대책본부는 오늘까지 수도권 교회와 관련한 코로나19 확진자는 총 63명이라고 밝혔으며  2차 감염자는 33명 이라고 전했습니다. \"\"\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9R3xTkxT1S4"
      },
      "source": [
        "데이터 전처리를 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTQUpKieSpdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc2577f-0bf6-4f19-f8b6-a23dbc5f7ae0"
      },
      "source": [
        "t = Tokenizer()\n",
        "t.fit_on_texts([text])\n",
        "vocab_size = len(t.word_index) + 1\n",
        "print('단어 집합의 크기 : %d' % vocab_size)\n",
        "\n",
        "\n",
        "print(t.word_index)\n",
        "\n",
        "sequences = list()\n",
        "for line in text.split('\\n'): \n",
        "    encoded = t.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(encoded)):\n",
        "        sequence = encoded[:i+1]\n",
        "        sequences.append(sequence)\n",
        "\n",
        "print('학습에 사용할 샘플의 개수: %d' % len(sequences))\n",
        "\n",
        "\n",
        "max_len=max(len(l) for l in sequences) \n",
        "\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "X = sequences[:,:-1]\n",
        "y = sequences[:,-1]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단어 집합의 크기 : 31\n",
            "{'2차': 1, '특히': 2, '최근': 3, '수도권에서는': 4, '교회': 5, '소모임': 6, '참석자에': 7, '이어': 8, '이들': 9, '가족과': 10, '지인으로': 11, '번지는': 12, '감염': 13, '사례가': 14, '증가하고': 15, '있습니다': 16, '중앙재난안전대책본부는': 17, '오늘까지': 18, '수도권': 19, '교회와': 20, '관련한': 21, '코로나19': 22, '확진자는': 23, '총': 24, '63명이라고': 25, '밝혔으며': 26, '감염자는': 27, '33명': 28, '이라고': 29, '전했습니다': 30}\n",
            "학습에 사용할 샘플의 개수: 30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A94dnqA8T8qg"
      },
      "source": [
        "Word Embedding 모델을 Keras로 정의하고 트레이닝을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3xzFR9gTpGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65500039-cebd-4a99-d2f5-765e1425aeb2"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=max_len-1))  \n",
        "model.add(SimpleRNN(32))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=120, verbose=2)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/120\n",
            "1/1 - 4s - loss: 3.4360 - accuracy: 0.0000e+00\n",
            "Epoch 2/120\n",
            "1/1 - 0s - loss: 3.4177 - accuracy: 0.0667\n",
            "Epoch 3/120\n",
            "1/1 - 0s - loss: 3.4003 - accuracy: 0.0667\n",
            "Epoch 4/120\n",
            "1/1 - 0s - loss: 3.3830 - accuracy: 0.1333\n",
            "Epoch 5/120\n",
            "1/1 - 0s - loss: 3.3650 - accuracy: 0.1667\n",
            "Epoch 6/120\n",
            "1/1 - 0s - loss: 3.3458 - accuracy: 0.1333\n",
            "Epoch 7/120\n",
            "1/1 - 0s - loss: 3.3253 - accuracy: 0.2000\n",
            "Epoch 8/120\n",
            "1/1 - 0s - loss: 3.3036 - accuracy: 0.2333\n",
            "Epoch 9/120\n",
            "1/1 - 0s - loss: 3.2808 - accuracy: 0.2333\n",
            "Epoch 10/120\n",
            "1/1 - 0s - loss: 3.2575 - accuracy: 0.2000\n",
            "Epoch 11/120\n",
            "1/1 - 0s - loss: 3.2343 - accuracy: 0.2000\n",
            "Epoch 12/120\n",
            "1/1 - 0s - loss: 3.2119 - accuracy: 0.1333\n",
            "Epoch 13/120\n",
            "1/1 - 0s - loss: 3.1907 - accuracy: 0.1333\n",
            "Epoch 14/120\n",
            "1/1 - 0s - loss: 3.1709 - accuracy: 0.1333\n",
            "Epoch 15/120\n",
            "1/1 - 0s - loss: 3.1513 - accuracy: 0.1333\n",
            "Epoch 16/120\n",
            "1/1 - 0s - loss: 3.1308 - accuracy: 0.1333\n",
            "Epoch 17/120\n",
            "1/1 - 0s - loss: 3.1094 - accuracy: 0.1667\n",
            "Epoch 18/120\n",
            "1/1 - 0s - loss: 3.0878 - accuracy: 0.2000\n",
            "Epoch 19/120\n",
            "1/1 - 0s - loss: 3.0663 - accuracy: 0.2333\n",
            "Epoch 20/120\n",
            "1/1 - 0s - loss: 3.0447 - accuracy: 0.2333\n",
            "Epoch 21/120\n",
            "1/1 - 0s - loss: 3.0227 - accuracy: 0.2667\n",
            "Epoch 22/120\n",
            "1/1 - 0s - loss: 3.0009 - accuracy: 0.2333\n",
            "Epoch 23/120\n",
            "1/1 - 0s - loss: 2.9800 - accuracy: 0.2667\n",
            "Epoch 24/120\n",
            "1/1 - 0s - loss: 2.9603 - accuracy: 0.3000\n",
            "Epoch 25/120\n",
            "1/1 - 0s - loss: 2.9411 - accuracy: 0.2667\n",
            "Epoch 26/120\n",
            "1/1 - 0s - loss: 2.9210 - accuracy: 0.2667\n",
            "Epoch 27/120\n",
            "1/1 - 0s - loss: 2.8994 - accuracy: 0.2333\n",
            "Epoch 28/120\n",
            "1/1 - 0s - loss: 2.8769 - accuracy: 0.2667\n",
            "Epoch 29/120\n",
            "1/1 - 0s - loss: 2.8549 - accuracy: 0.3000\n",
            "Epoch 30/120\n",
            "1/1 - 0s - loss: 2.8341 - accuracy: 0.3333\n",
            "Epoch 31/120\n",
            "1/1 - 0s - loss: 2.8136 - accuracy: 0.3333\n",
            "Epoch 32/120\n",
            "1/1 - 0s - loss: 2.7921 - accuracy: 0.3667\n",
            "Epoch 33/120\n",
            "1/1 - 0s - loss: 2.7691 - accuracy: 0.4000\n",
            "Epoch 34/120\n",
            "1/1 - 0s - loss: 2.7458 - accuracy: 0.4000\n",
            "Epoch 35/120\n",
            "1/1 - 0s - loss: 2.7240 - accuracy: 0.3333\n",
            "Epoch 36/120\n",
            "1/1 - 0s - loss: 2.7018 - accuracy: 0.3333\n",
            "Epoch 37/120\n",
            "1/1 - 0s - loss: 2.6789 - accuracy: 0.4667\n",
            "Epoch 38/120\n",
            "1/1 - 0s - loss: 2.6554 - accuracy: 0.4333\n",
            "Epoch 39/120\n",
            "1/1 - 0s - loss: 2.6335 - accuracy: 0.5000\n",
            "Epoch 40/120\n",
            "1/1 - 0s - loss: 2.6101 - accuracy: 0.5000\n",
            "Epoch 41/120\n",
            "1/1 - 0s - loss: 2.5874 - accuracy: 0.6000\n",
            "Epoch 42/120\n",
            "1/1 - 0s - loss: 2.5646 - accuracy: 0.5667\n",
            "Epoch 43/120\n",
            "1/1 - 0s - loss: 2.5413 - accuracy: 0.5667\n",
            "Epoch 44/120\n",
            "1/1 - 0s - loss: 2.5204 - accuracy: 0.5667\n",
            "Epoch 45/120\n",
            "1/1 - 0s - loss: 2.4982 - accuracy: 0.5667\n",
            "Epoch 46/120\n",
            "1/1 - 0s - loss: 2.4777 - accuracy: 0.5667\n",
            "Epoch 47/120\n",
            "1/1 - 0s - loss: 2.4524 - accuracy: 0.6000\n",
            "Epoch 48/120\n",
            "1/1 - 0s - loss: 2.4333 - accuracy: 0.6000\n",
            "Epoch 49/120\n",
            "1/1 - 0s - loss: 2.4147 - accuracy: 0.5667\n",
            "Epoch 50/120\n",
            "1/1 - 0s - loss: 2.3895 - accuracy: 0.6000\n",
            "Epoch 51/120\n",
            "1/1 - 0s - loss: 2.3732 - accuracy: 0.6000\n",
            "Epoch 52/120\n",
            "1/1 - 0s - loss: 2.3566 - accuracy: 0.6000\n",
            "Epoch 53/120\n",
            "1/1 - 0s - loss: 2.3292 - accuracy: 0.6000\n",
            "Epoch 54/120\n",
            "1/1 - 0s - loss: 2.3354 - accuracy: 0.6000\n",
            "Epoch 55/120\n",
            "1/1 - 0s - loss: 2.3053 - accuracy: 0.6000\n",
            "Epoch 56/120\n",
            "1/1 - 0s - loss: 2.3072 - accuracy: 0.5667\n",
            "Epoch 57/120\n",
            "1/1 - 0s - loss: 2.2906 - accuracy: 0.6000\n",
            "Epoch 58/120\n",
            "1/1 - 0s - loss: 2.2630 - accuracy: 0.6333\n",
            "Epoch 59/120\n",
            "1/1 - 0s - loss: 2.2339 - accuracy: 0.6000\n",
            "Epoch 60/120\n",
            "1/1 - 0s - loss: 2.2089 - accuracy: 0.6000\n",
            "Epoch 61/120\n",
            "1/1 - 0s - loss: 2.2072 - accuracy: 0.6000\n",
            "Epoch 62/120\n",
            "1/1 - 0s - loss: 2.1882 - accuracy: 0.6000\n",
            "Epoch 63/120\n",
            "1/1 - 0s - loss: 2.1566 - accuracy: 0.6333\n",
            "Epoch 64/120\n",
            "1/1 - 0s - loss: 2.1446 - accuracy: 0.6333\n",
            "Epoch 65/120\n",
            "1/1 - 0s - loss: 2.1271 - accuracy: 0.6333\n",
            "Epoch 66/120\n",
            "1/1 - 0s - loss: 2.1124 - accuracy: 0.6000\n",
            "Epoch 67/120\n",
            "1/1 - 0s - loss: 2.0946 - accuracy: 0.6000\n",
            "Epoch 68/120\n",
            "1/1 - 0s - loss: 2.0713 - accuracy: 0.6333\n",
            "Epoch 69/120\n",
            "1/1 - 0s - loss: 2.0552 - accuracy: 0.6667\n",
            "Epoch 70/120\n",
            "1/1 - 0s - loss: 2.0380 - accuracy: 0.6667\n",
            "Epoch 71/120\n",
            "1/1 - 0s - loss: 2.0257 - accuracy: 0.6667\n",
            "Epoch 72/120\n",
            "1/1 - 0s - loss: 2.0057 - accuracy: 0.6667\n",
            "Epoch 73/120\n",
            "1/1 - 0s - loss: 1.9857 - accuracy: 0.6667\n",
            "Epoch 74/120\n",
            "1/1 - 0s - loss: 1.9707 - accuracy: 0.6667\n",
            "Epoch 75/120\n",
            "1/1 - 0s - loss: 1.9561 - accuracy: 0.7000\n",
            "Epoch 76/120\n",
            "1/1 - 0s - loss: 1.9359 - accuracy: 0.6667\n",
            "Epoch 77/120\n",
            "1/1 - 0s - loss: 1.9172 - accuracy: 0.7333\n",
            "Epoch 78/120\n",
            "1/1 - 0s - loss: 1.9002 - accuracy: 0.7333\n",
            "Epoch 79/120\n",
            "1/1 - 0s - loss: 1.8844 - accuracy: 0.7333\n",
            "Epoch 80/120\n",
            "1/1 - 0s - loss: 1.8680 - accuracy: 0.7667\n",
            "Epoch 81/120\n",
            "1/1 - 0s - loss: 1.8480 - accuracy: 0.7333\n",
            "Epoch 82/120\n",
            "1/1 - 0s - loss: 1.8323 - accuracy: 0.7000\n",
            "Epoch 83/120\n",
            "1/1 - 0s - loss: 1.8169 - accuracy: 0.7333\n",
            "Epoch 84/120\n",
            "1/1 - 0s - loss: 1.7999 - accuracy: 0.8000\n",
            "Epoch 85/120\n",
            "1/1 - 0s - loss: 1.7827 - accuracy: 0.7333\n",
            "Epoch 86/120\n",
            "1/1 - 0s - loss: 1.7676 - accuracy: 0.7667\n",
            "Epoch 87/120\n",
            "1/1 - 0s - loss: 1.7609 - accuracy: 0.7333\n",
            "Epoch 88/120\n",
            "1/1 - 0s - loss: 1.7604 - accuracy: 0.7667\n",
            "Epoch 89/120\n",
            "1/1 - 0s - loss: 1.7265 - accuracy: 0.8000\n",
            "Epoch 90/120\n",
            "1/1 - 0s - loss: 1.7103 - accuracy: 0.8000\n",
            "Epoch 91/120\n",
            "1/1 - 0s - loss: 1.6921 - accuracy: 0.8333\n",
            "Epoch 92/120\n",
            "1/1 - 0s - loss: 1.6825 - accuracy: 0.8333\n",
            "Epoch 93/120\n",
            "1/1 - 0s - loss: 1.6585 - accuracy: 0.8333\n",
            "Epoch 94/120\n",
            "1/1 - 0s - loss: 1.6498 - accuracy: 0.8000\n",
            "Epoch 95/120\n",
            "1/1 - 0s - loss: 1.6304 - accuracy: 0.8333\n",
            "Epoch 96/120\n",
            "1/1 - 0s - loss: 1.6142 - accuracy: 0.9000\n",
            "Epoch 97/120\n",
            "1/1 - 0s - loss: 1.6022 - accuracy: 0.9000\n",
            "Epoch 98/120\n",
            "1/1 - 0s - loss: 1.5814 - accuracy: 0.9000\n",
            "Epoch 99/120\n",
            "1/1 - 0s - loss: 1.5683 - accuracy: 0.9000\n",
            "Epoch 100/120\n",
            "1/1 - 0s - loss: 1.5527 - accuracy: 0.9000\n",
            "Epoch 101/120\n",
            "1/1 - 0s - loss: 1.5361 - accuracy: 0.9333\n",
            "Epoch 102/120\n",
            "1/1 - 0s - loss: 1.5242 - accuracy: 0.9333\n",
            "Epoch 103/120\n",
            "1/1 - 0s - loss: 1.5053 - accuracy: 0.9333\n",
            "Epoch 104/120\n",
            "1/1 - 0s - loss: 1.4947 - accuracy: 0.9000\n",
            "Epoch 105/120\n",
            "1/1 - 0s - loss: 1.4753 - accuracy: 0.9333\n",
            "Epoch 106/120\n",
            "1/1 - 0s - loss: 1.4635 - accuracy: 0.9333\n",
            "Epoch 107/120\n",
            "1/1 - 0s - loss: 1.4459 - accuracy: 0.9333\n",
            "Epoch 108/120\n",
            "1/1 - 0s - loss: 1.4336 - accuracy: 0.9333\n",
            "Epoch 109/120\n",
            "1/1 - 0s - loss: 1.4166 - accuracy: 0.9333\n",
            "Epoch 110/120\n",
            "1/1 - 0s - loss: 1.4032 - accuracy: 0.9333\n",
            "Epoch 111/120\n",
            "1/1 - 0s - loss: 1.3888 - accuracy: 0.9333\n",
            "Epoch 112/120\n",
            "1/1 - 0s - loss: 1.3728 - accuracy: 0.9333\n",
            "Epoch 113/120\n",
            "1/1 - 0s - loss: 1.3609 - accuracy: 0.9333\n",
            "Epoch 114/120\n",
            "1/1 - 0s - loss: 1.3453 - accuracy: 0.9333\n",
            "Epoch 115/120\n",
            "1/1 - 0s - loss: 1.3306 - accuracy: 0.9333\n",
            "Epoch 116/120\n",
            "1/1 - 0s - loss: 1.3180 - accuracy: 0.9333\n",
            "Epoch 117/120\n",
            "1/1 - 0s - loss: 1.3051 - accuracy: 0.9333\n",
            "Epoch 118/120\n",
            "1/1 - 0s - loss: 1.2920 - accuracy: 0.9333\n",
            "Epoch 119/120\n",
            "1/1 - 0s - loss: 1.2770 - accuracy: 0.9667\n",
            "Epoch 120/120\n",
            "1/1 - 0s - loss: 1.2617 - accuracy: 0.9667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f39b0018710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is_TW5rxUSla"
      },
      "source": [
        "##문제2-1~3. 이 셀에서 3 곳의 your_code_here를 찾아서 코드를 완성하십시오.(15점)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLQpmYxSTqN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "263fdfe0-365f-401d-a437-65ba857cf3ad"
      },
      "source": [
        "# 이 블록에서 yoou_code_here 부분을 완성하시오\n",
        "def sentence_generation(model, t, current_word, n): # model = 모델, t = 토크나이저, current_word = 현재 단어, n = 반복할 횟수\n",
        "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
        "    sentence = ''\n",
        "    for _ in range(n): # n번 반복\n",
        "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
        "        encoded = pad_sequences([encoded], maxlen=30, padding='pre') # 데이터에 대한 패딩\n",
        "        result = np.argmax(model.predict(encoded), axis=-1)\n",
        "        for word, index in t.word_index.items(): \n",
        "            if index == result: # 만약 예측한 단어의 인덱스 값이 동일한 단어가 있다면\n",
        "                break # 해당 단어가 예측 단어이므로 break\n",
        "        # 아래 부분의 코딩을 완성하십시오        \n",
        "        current_word = current_word + ' '  + word  # 현재 단어 + ' ' + 예측한 단어를 current_word로 저장(5점)\n",
        "        sentence = sentence + ' ' + word  # 예측한 단어를 sentence에 append(5점)\n",
        "     \n",
        "    sentence = init_word + sentence # 초기의 텍스트와 for loop에서 생성된 텍스트를 concatenate(5점)\n",
        "    return sentence\n",
        "\n",
        "print(sentence_generation(model, t, '특히', 10))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "특히 수도권에서는 수도권에서는 교회 소모임 참석자에 이어 이들 가족과 지인으로 있습니다\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXAXa6AXrqbU"
      },
      "source": [
        "#문제3 : Keras tuner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVsWQo8DVNsL"
      },
      "source": [
        "from tensorflow.keras import applications\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLIiyRQ4NJ6W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38c6a13-ef6d-4967-d849-5ab8f2cbc380"
      },
      "source": [
        "!pip install -q -U keras-tuner\n",
        "import kerastuner as kt"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10kB 24.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 29.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 35.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 30.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 32.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 9.1MB/s \n",
            "\u001b[?25h  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c73LL_nXQ2de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a62a4f6-938c-4216-beca-002b327b8520"
      },
      "source": [
        "# load CIFAR-10 dataset\n",
        "(img_train, y_train), (img_test, y_test) = keras.datasets.cifar10.load_data()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyi1MLXnSg7q"
      },
      "source": [
        "from tensorflow.keras import backend\n",
        "# Normalize pixel values between 0 and 1\n",
        "x_train = img_train.astype('float32') / 255.0\n",
        "x_test = img_test.astype('float32') / 255.0\n",
        "\n",
        "img_rows, img_cols = 32,32\n",
        "if backend.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n",
        "    input_shape = (img_rows, img_cols, 3)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEqPtul5i5mK"
      },
      "source": [
        "##문제3-1. 전이학습 : VGG16을 pre-trained 모델로  사용할 수 있도록 다음 셀의 코드를 완성하십시오.(5점)\n",
        "아래의 셀에서는 VGG16의 Pre-trained 모델을 사용합니다. 이 방법에 대하여는 P11-2 VGG16실습을 참고하십시오.\n",
        "### 참고 https://keras.io/ko/applications/#vgg16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot6sWWoVK-Nb"
      },
      "source": [
        "# define the model\n",
        "def model_builder(hp):\n",
        "    base_model = applications.VGG16(weights='imagenet', pooling='avg', include_top=False) # your_code_here에 적절한 값을 넣으십시오.\n",
        "    inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "    x = base_model(inputs)\n",
        "    for i in range(hp.Int('num_layers', min_value=2, max_value=5, step=1)):\n",
        "        x = Dense(units=hp.Int('units' + str(i), min_value=32, max_value=128, step=32), activation='relu')(x)\n",
        "    x = Dense(10, activation='softmax')(x) \n",
        "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qLUZsnImR8p"
      },
      "source": [
        "##문제3-2. 아래 셀에 tuner 오브젝트 생성하는 코드를 완성하시오. 이 튜너는 RandomSearch를 사용하고 그를 위한 인수값은 다음을 사용하시오.(5점)\n",
        "\n",
        "\n",
        "    objective='val_accuracy',\n",
        "    max_trials=3,\n",
        "    executions_per_trial=1,\n",
        "    directory='tuning_dir',\n",
        "    project_name='ktuner'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_GUsErT17u1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "1db60db5-5b1d-4118-83a8-2e77200e74e1"
      },
      "source": [
        "tuner = RandomSearch(\n",
        "    objective='val_accuracy',\n",
        "  max_trials=3,\n",
        "  executions_per_trial=1,\n",
        "  directory='tuning_dir',\n",
        "  project_name='ktuner')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-7ff12d4ccf5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mexecutions_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mdirectory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tuning_dir'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   project_name='ktuner')\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'hypermodel'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2g2WQgc5tdo"
      },
      "source": [
        "#Run the hyperparameter search. The arguments for the search method are the same as those used for tf.keras.model.fit in addition to the callback above.\n",
        "%time tuner.search(x_train, y_train, epochs = 1, validation_data = (x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlTbmaKYStFL"
      },
      "source": [
        "##문제3-3. 최적의 하이퍼 파라미터들의 조합을 print문으로 출력하시오. (5점)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "kCjyCGlyOgEe",
        "outputId": "79adddee-8b15-42e3-ffa8-45c6f6e56c07"
      },
      "source": [
        "print(tuner.get_best_models) #아래에 예시한 하이퍼파라미터들을 출력하시오."
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-a7fdd4e514be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_models\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#아래에 예시한 하이퍼파라미터들을 출력하시오.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tuner' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2_mC3_7Se80"
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yqKdFCnR_kJ",
        "outputId": "b007ffff-508f-48fd-bda5-fa1896465413"
      },
      "source": [
        "tuner.get_best_models()[0].summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
            "_________________________________________________________________\n",
            "vgg16 (Functional)           (None, 512)               14714688  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 96)                49248     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               12416     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                4128      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 14,781,866\n",
            "Trainable params: 14,781,866\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNlFFkFOijfn"
      },
      "source": [
        "#코딩 테스트를 완료하였습니다. 1학기-기말고사-yourname.ipynb로 다운로드받아서 LMS와 깃헙에 과제물로 제출해주십시오.\n",
        "#수고 하였습니다!"
      ]
    }
  ]
}